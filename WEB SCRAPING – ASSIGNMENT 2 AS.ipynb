{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing selenium librariy\n",
    "!pip install selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e316a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d92f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.naukri.com/\n",
    "#2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data\n",
    "\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "\n",
    "# getting the webpage https://www.naukri.com/\n",
    "driver.get('https://www.naukri.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "#Entering “Data Analyst” in “Skill, Designations, Companies” field \n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input \") \n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "\n",
    "#Entering “Bangalore” in “enter the location” field \n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[5]/div/div/div/input\") \n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_class_name('qsbSubmit') \n",
    "search_button.click()\n",
    "\n",
    "\n",
    "# empty list for required data\n",
    "Job_title=[]\n",
    "Comp_Name=[]\n",
    "Location=[]\n",
    "Experiance=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236735ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Comp_Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experiance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Job_title, Comp_Name, Location, Experiance]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrapeing the data for the 1st 10jobs results\n",
    "\n",
    "#scraping Job title\n",
    "title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\") \n",
    "for i in title:\n",
    "    Job_title.append(i.text)\n",
    "print(Job_title[0:10])\n",
    "\n",
    "#scraping Company Name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\") \n",
    "for i in name:\n",
    "    Comp_Name.append(i.text)\n",
    "print(Comp_Name[0:10])\n",
    "\n",
    "#scraping Job location\n",
    "loc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\") \n",
    "for i in loc:\n",
    "    Location.append(i.text)\n",
    "print(Location[0:10])\n",
    "\n",
    "#scraping experiance\n",
    "exp=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\") \n",
    "for i in exp:\n",
    "    Experiance.append(i.text)\n",
    "print(Experiance[0:10])\n",
    "\n",
    "#createing a dataframe of the scraped data\n",
    "Ten_jobs_results=pd.DataFrame({'Job_title':Job_title[0:10],'Comp_Name':Comp_Name[0:10],'Location':Location[0:10],'Experiance':Experiance[0:10]})\n",
    "Ten_jobs_results\n",
    "\n",
    "#closeing window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cfa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d944f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "#1. First get the webpage https://www.naukri.com/\n",
    "#2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# getting the webpage https://www.naukri.com/\n",
    "driver.get('https://www.naukri.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "#Entering “Data Scientist” in “Skill, Designations, Companies” field \n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input \") \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "#Entering “Bangalore” in “enter the location” field \n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[5]/div/div/div/input\") \n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_class_name('qsbSubmit') \n",
    "search_button.click()\n",
    "\n",
    "#creating empty lits\n",
    "Job_title=[]\n",
    "Comp_Name=[]\n",
    "Location=[]\n",
    "Experiance=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapeing the data for the 1st 10jobs results\n",
    "\n",
    "#scraping Job title\n",
    "title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\") \n",
    "for i in title:\n",
    "    Job_title.append(i.text)\n",
    "print(Job_title[0:10])\n",
    "\n",
    "#scraping Company Name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\") \n",
    "for i in name:\n",
    "    Comp_Name.append(i.text)\n",
    "print(Comp_Name[0:10])\n",
    "\n",
    "#scraping Job location\n",
    "loc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\") \n",
    "for i in loc:\n",
    "    Location.append(i.text)\n",
    "print(Location[0:10])\n",
    "\n",
    "#scraping experiance\n",
    "exp=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\") \n",
    "for i in exp:\n",
    "    Experiance.append(i.text)\n",
    "print(Experiance[0:10])\n",
    "\n",
    "#createing a dataframe of the scraped data\n",
    "Ten_jobs_results=pd.DataFrame({'Job_title':Job_title[0:10],'Comp_Name':Comp_Name[0:10],'Location':Location[0:10],'Experiance':Experiance[0:10]})\n",
    "Ten_jobs_results\n",
    "\n",
    "#closing the window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c4a3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "#The task will be done as shown in the below steps:\n",
    "#1. first get the webpage https://www.naukri.com/\n",
    "#2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "#5. Then scrape the data for the first 10 jobs results you get.\n",
    "#6. Finally create a dataframe of the scraped data.\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# getting the webpage https://www.naukri.com/\n",
    "driver.get('https://www.naukri.com/')\n",
    "driver.maximize_window()\n",
    "\n",
    "#Entering “Data Scientist” in “Skill, Designations, Companies” field \n",
    "search_field_designation=driver.find_element_by_class_name(\"suggestor-input \") \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_class_name('qsbSubmit') \n",
    "search_button.click()\n",
    "\n",
    "#Creating empty lits\n",
    "Job_title=[]\n",
    "Comp_Name=[]\n",
    "Location=[]\n",
    "Experiance=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dae908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the location filter \n",
    "loc=driver.find_elements_by_xpath('//div[@class=\"mt-8 chckBoxCont\"]//span') \n",
    "for i in loc:\n",
    "    if i.text==('Delhi / NCR'):\n",
    "        i.click()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the salary filter \n",
    "sal=driver.find_elements_by_xpath('//div[@class=\"mt-8 chckBoxCont\"]//span') \n",
    "for i in sal:\n",
    "    if i.text==('3-6 Lakhs'):\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ae849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping Job title\n",
    "title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\") \n",
    "for i in title:\n",
    "    Job_title.append(i.text)\n",
    "print(Job_title[0:10])\n",
    "\n",
    "#scraping Company Name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\") \n",
    "for i in name:\n",
    "    Comp_Name.append(i.text)\n",
    "print(Comp_Name[0:10])\n",
    "\n",
    "#scraping Job location\n",
    "loc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\") \n",
    "for i in loc:\n",
    "    Location.append(i.text)\n",
    "print(Location[0:10])\n",
    "\n",
    "#scraping experiance\n",
    "exp=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\") \n",
    "for i in exp:\n",
    "    Experiance.append(i.text)\n",
    "print(Experiance[0:10])\n",
    "\n",
    "#createing a dataframe of the scraped data\n",
    "ten_jobs_results=pd.DataFrame({'Job_title':Job_title[0:10],'Comp_Name':Comp_Name[0:10],'Location':Location[0:10],'Experiance':Experiance[0:10]})\n",
    "ten_jobs_results\n",
    "\n",
    "#closing the window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19c24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#1. Brand 2. Product Description 3. Price The attributes which you have to scrape is ticked marked in the below image.\n",
    "#To scrape the data you have to go through following steps:\n",
    "#1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "#2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "#3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "#4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "#5. Now scrape data from this page as usual #6. Repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# Flipkart webpage by url : https://www.flipkart.com/\n",
    "url=\"https://www.flipkart.com/\" \n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window() \n",
    "\n",
    "# closing login pop-up window\n",
    "Close_button=driver.find_element_by_xpath('/html/body/div[2]/div/div/button') \n",
    "Close_button.click()\n",
    "\n",
    "#Entering “sunglasses” in search field where “search for products, brands and more” \n",
    "search_field_sunglasses=driver.find_element_by_class_name(\"_3704LK \") \n",
    "search_field_sunglasses.send_keys(\"sunglasses\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_class_name('_34RNph') \n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Creating empty lits\n",
    "Brand=[]\n",
    "Product_Desp=[]\n",
    "Price=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85682649",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):    \n",
    "    brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    product_desp=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for i in product_desp:\n",
    "        Product_Desp.append(i.text)\n",
    "        \n",
    "    price=driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div[1]\")\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    next_button=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a/span\")\n",
    "    next_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "next_button=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[3]\")\n",
    "next_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "brand1=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "for i in brand1:\n",
    "    Brand.append(i.text[:20])\n",
    "        \n",
    "product_desp1=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "for i in product_desp1:\n",
    "    Product_Desp.append(i.text[:20])\n",
    "        \n",
    "price=driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div[1]\")\n",
    "for i in price:\n",
    "    Price.append(i.text[:20])\n",
    "    \n",
    "print(Brand[:100])\n",
    "print(Product_Desp[:100])\n",
    "print(Price[:100])\n",
    "\n",
    "first_100sunglasses=pd.DataFrame({'Brand':Brand[:100],'Product_Desp':Product_Desp[:100],'Price':Price[:100]})\n",
    "first_100sunglasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close window\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4fd91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.This task will be done in following steps:\n",
    "#1. First get the webpage https://www.flipkart.com/ #2. Enter “iphone 11” in “Search” field . #3. Then click the search button As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "#1. Rating 2. Review summary 3. Full review 4. You have to scrape this data for first 100 reviews.\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# Flipkart webpage by url : https://www.flipkart.com/\n",
    "url=\"https://www.flipkart.com/\" \n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window() \n",
    "\n",
    "# closing login pop-up window\n",
    "Close_button=driver.find_element_by_xpath('/html/body/div[2]/div/div/button') \n",
    "Close_button.click()\n",
    "\n",
    "#Entering “iphone 11” in search field where “search for products, brands and more” \n",
    "search_field_iphone=driver.find_element_by_class_name(\"_3704LK \") \n",
    "search_field_iphone.send_keys(\"iphone 11\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_class_name('_34RNph') \n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Creating empty lits\n",
    "Rating=[]\n",
    "review_sum=[]\n",
    "full_review=[]\n",
    "\n",
    "#opneing full rating and review tab\n",
    "driver.close()\n",
    "driver1=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "driver1.get('https://www.flipkart.com/apple-iphone-11-white-128-gb/product-reviews/itme32df47ea6742?pid=MOBFWQ6B7KKRXDDS&lid=LSTMOBFWQ6B7KKRXDDSMBIFYZ&marketplace=FLIPKART')\n",
    "driver1.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe83ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_100():\n",
    "    rating=driver1.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for i in rating:\n",
    "        Rating.append(i.text)\n",
    "    \n",
    "    a=driver1.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for i in a:\n",
    "        review_sum.append(i.text)\n",
    "\n",
    "    a=driver1.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for i in a:\n",
    "        full_review.append(i.text)\n",
    "flipkart_100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e479c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(0,9):\n",
    "    if a==0:\n",
    "        next=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[3]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)\n",
    "    elif a==1:\n",
    "        next1=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[4]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)\n",
    "    elif a==2:\n",
    "        next2=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[5]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)\n",
    "    elif a==3:\n",
    "        next3=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[6]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)\n",
    "    elif a==4:\n",
    "        next4=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[7]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)    \n",
    "    elif a==5:\n",
    "        next5=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[8]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)     \n",
    "    elif a==6:\n",
    "        next6=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[9]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)     \n",
    "    elif a==7:\n",
    "        next7=driver1.find_element_by_xpath('/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[10]').click()\n",
    "        flipkart_100()\n",
    "        time.sleep(5)\n",
    "driver1.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816906a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final=pd.DataFrame({'Ratings':Rating,'Review_sum':review_sum,'Full_review':full_review})\n",
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54232365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7953168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.You have to scrape 4 attributes of each sneaker:\n",
    "#1. Brand 2. Product Description 3. Price\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# Flipkart webpage by url : https://www.flipkart.com/\n",
    "url=\"https://www.flipkart.com/\" \n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window() \n",
    "\n",
    "# closing login pop-up window\n",
    "Close_button=driver.find_element_by_xpath('/html/body/div[2]/div/div/button') \n",
    "Close_button.click()\n",
    "\n",
    "#Entering “sneakers” in search field where “search for products, brands and more” \n",
    "search_field_iphone=driver.find_element_by_class_name(\"_3704LK \") \n",
    "search_field_iphone.send_keys(\"sneakers\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_class_name('_34RNph') \n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Creating empty lits\n",
    "Brand=[]\n",
    "Product_Desp=[]\n",
    "Price=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):    \n",
    "    brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    product_desp=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for i in product_desp:\n",
    "        Product_Desp.append(i.text)\n",
    "        \n",
    "    price=driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div[1]\")\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    next_button=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a/span\")\n",
    "    next_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "next_button=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[3]\")\n",
    "next_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "brand1=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "for i in brand1:\n",
    "    Brand.append(i.text[:20])\n",
    "        \n",
    "product_desp1=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "for i in product_desp1:\n",
    "    Product_Desp.append(i.text[:20])\n",
    "        \n",
    "price=driver.find_elements_by_xpath(\"//a[@class='_3bPFwb']/div[1]\")\n",
    "for i in price:\n",
    "    Price.append(i.text[:20])\n",
    "    \n",
    "print(Brand[:100])\n",
    "print(Product_Desp[:100])\n",
    "print(Price[:100])\n",
    "\n",
    "first_100sneakers=pd.DataFrame({'Brand':Brand[:100],'Product_Desp':Product_Desp[:100],'Price':Price[:100]})\n",
    "first_100sneakers\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08ed53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5fa39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: Go to the link - https://www.myntra.com/shoes Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "# And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"disable--infobars\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# myntra webpage: https://www.myntra.com/shoes\n",
    "url=\"https://www.myntra.com/shoes\" \n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window() \n",
    "\n",
    "#Creating empty lits\n",
    "Brand=[]\n",
    "Product_Desp=[]\n",
    "Price=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying second Price filter \n",
    "price_filter=driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]/label[@class='common-customCheckbox vertical-filters-label']//div[@class='common-checkboxIndicator']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Color filter to “Black”\n",
    "color_filter=driver.find_element_by_xpath(\"//ul/li[1][@class='colour-listItem']/label[1]/div\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2848a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):    \n",
    "    brand=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    product_desp=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4[@class='product-product']\")\n",
    "    for i in product_desp:\n",
    "        Product_Desp.append(i.text)\n",
    "        \n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='product-price']/span\")\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    next_button=driver.find_element_by_xpath(\"//li[@class='pagination-next']/a\").click()\n",
    "        \n",
    "print(Brand[:100])\n",
    "print(Product_Desp[:100])\n",
    "print(Price[:100])\n",
    "\n",
    "first_100shoes=pd.DataFrame({'Brand':Brand[:100],'Product_Desp':Product_Desp[:100],'Price':Price[:100]})\n",
    "first_100shoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c54046",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8: Go to webpage https://www.amazon.in/  Enter “Laptop” in the search field and then click the search icon.\n",
    "# Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "# After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "#1. Title 2. Ratings 3. Price\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# opneing webpage https://www.amazon.in/\n",
    "url=\"https://www.amazon.in/\" \n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window() \n",
    "\n",
    "#Entering “Laptop” in search field where “search for products, brands and more” \n",
    "search_field=driver.find_element_by_xpath(\"//div//div[@class='nav-search-field ']/input[@id='twotabsearchtextbox']\")\n",
    "search_field.send_keys(\"Laptop\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\") \n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Creating empty lits\n",
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]\n",
    "\n",
    "#Filtering CPU\n",
    "cpu_filter=driver.find_element_by_xpath(\"//div[@id='filters']/ul/li[@aria-label='Intel Core i7']/span/a/div\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfbc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "for a in t:\n",
    "    Title.append(a.text)\n",
    "print(Title[:10])\n",
    "\n",
    "R_box=driver.find_elements_by_xpath(\"//div[@class='a-row a-size-small']/span[1]\")\n",
    "for i in R_box:\n",
    "    Rating.append(i.get_attribute('aria-label'))\n",
    "print(Rating[:10])\n",
    "\n",
    "t=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for a in t:\n",
    "    Price.append(a.text)\n",
    "print(Price[:10])\n",
    "\n",
    "first_10laptop=pd.DataFrame({'Title':Title[:10],'Rating':Rating[:10],'Price':Price[:10]})\n",
    "first_10laptop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04872998",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f8085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida \n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. \n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af252a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# opneing webpage https://www.ambitionbox.com/\n",
    "url=\"https://www.ambitionbox.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "#Job Option\n",
    "job_option=driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[6]').click()\n",
    "\n",
    "#Entering “Data Scientist” in search field where “search by Designations, Companies, Skills” \n",
    "search_field=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input\")\n",
    "search_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "# clicking on search button.\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\") \n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "#applying location filter\n",
    "loc=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/i\").click()\n",
    "\n",
    "#sending data in search filter\n",
    "search_field=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input\")\n",
    "search_field.send_keys(\"Noida\")\n",
    "time.sleep(2)\n",
    "\n",
    "filter=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdfece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "comp_name=[]\n",
    "time=[]\n",
    "Rating=[]\n",
    "\n",
    "name=driver.find_elements_by_xpath(\"//div[@class='company-info']/p\")\n",
    "for i in name:\n",
    "    comp_name.append(i.text)\n",
    "print(comp_name)\n",
    "\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='rating-wrapper']/a[1]\")\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "print(Rating)\n",
    "\n",
    "t=driver.find_elements_by_xpath(\"//div[@class='other-info']/span[1]\")\n",
    "for i in t:\n",
    "    time.append(i.text)\n",
    "print(time) \n",
    "df=pd.DataFrame({'comp_name':comp_name[:10],'Rating':Rating[:10],'time':time[:10]})\n",
    "df.drop(index=0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10: Write a python program to scrape the salary data for Data Scientist designation.You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. \n",
    "#The above task will be, done as shown in the below steps: 1. First get the webpage https://www.ambitionbox.com/\n",
    "#2. Click on the salaries option as shown in the image 3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and \n",
    "#then click on “Data Scientist”. 4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average \n",
    "#salary, minimum salary, maximum salary, experience required. 5. Store the data in a dataframe.\n",
    "\n",
    "#connect to web driver\n",
    "chrome_options=Options()\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\shaha\\Desktop\\My Course\\chromedriver_win32\\chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "# opneing webpage https://www.ambitionbox.com/\n",
    "url=\"https://www.ambitionbox.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "#maximize the chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "#salary Option\n",
    "salary_option=driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[4]').click()\n",
    "\n",
    "#Entering “Data Scientist” in search field where “search by Designations, Companies, Skills” \n",
    "search_field=driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input\")\n",
    "search_field.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a929d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_name=[]\n",
    "salry_record=[]\n",
    "avg_salary=[]\n",
    "min_salary=[]\n",
    "max_salary=[]\n",
    "exp=[]\n",
    "\n",
    "name=driver.find_elements_by_xpath(\"//div[@class='name']/a\")\n",
    "for i in name:\n",
    "    comp_name.append(i.text)\n",
    "print(comp_name)\n",
    "\n",
    "r=driver.find_elements_by_xpath(\"//div[@class='name']/span\")\n",
    "for i in r:\n",
    "    salry_record.append(i.text)\n",
    "print(salry_record)\n",
    "\n",
    "r=driver.find_elements_by_xpath(\"//p[@class='averageCtc']\")\n",
    "for i in r:\n",
    "    avg_salary.append(i.text)\n",
    "print(avg_salary)\n",
    "\n",
    "r=driver.find_elements_by_xpath(\"//div[@class='salary-values']/div[1]\")\n",
    "for i in r:\n",
    "    min_salary.append(i.text)\n",
    "print(min_salary)\n",
    "\n",
    "r=driver.find_elements_by_xpath(\"//div[@class='salary-values']/div[2]\")\n",
    "for i in r:\n",
    "    max_salary.append(i.text)\n",
    "print(max_salary)\n",
    "\n",
    "r=driver.find_elements_by_xpath(\"//div[@class='salaries sbold-list-header']\")\n",
    "for i in r:\n",
    "    exp.append(i.text[19:])\n",
    "print(exp)\n",
    "\n",
    "df=pd.DataFrame({'comp_name':comp_name,'salry_record':salry_record,'avg_salary':avg_salary,'min_salary':min_salary,'max_salary':max_salary,'exp':exp})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526afaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c411cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db46f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb1054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbe2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e5d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e486470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
